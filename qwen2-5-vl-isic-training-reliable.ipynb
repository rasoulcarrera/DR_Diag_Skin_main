{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen 2.5-2B VL Training on HAM10000 Dataset\n",
    "\n",
    "**Streamlined training pipeline for Kaggle environment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install transformers accelerate peft tqdm pillow datasets pandas bitsandbytes --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import libraries and configuration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    Qwen2VLProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset as HFDataset\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    \"train_image_dir\": \"/kaggle/input/small-isic\",\n",
    "    \"spatial_dataset_file\": \"./ham10000_with_spatial_data.json\",\n",
    "    \"output_dir\": \"./qwen2_5_vl_trained\",\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"max_length\": 512,\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Spatial awareness settings\n",
    "    \"include_spatial_descriptions\": True,\n",
    "    \"spatial_description_ratio\": 0.2,\n",
    "    \n",
    "    # Additional stability settings\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "# Apply QLoRA with gradient fix\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "#     lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "#     r=CONFIG[\"lora_r\"],\n",
    "#     bias=\"none\",\n",
    "#     target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "#     task_type=TaskType.CAUSAL_LM,\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "# Fix gradient computation\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'lora' in name:\n",
    "#         param.requires_grad = True\n",
    "\n",
    "print(\"Model loaded ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load spatial dataset\n",
    "def load_spatial_dataset(limit=None):\n",
    "    import json\n",
    "    import random\n",
    "    \n",
    "    # Load annotated data with spatial information\n",
    "    with open(CONFIG[\"spatial_dataset_file\"], 'r') as f:\n",
    "        spatial_data = json.load(f)\n",
    "    \n",
    "    image_dir = Path(CONFIG[\"train_image_dir\"])\n",
    "    conversations = []\n",
    "    \n",
    "    for item in spatial_data:\n",
    "        image_id = item['image_id']\n",
    "        \n",
    "        # Find image file\n",
    "        image_path = None\n",
    "        for ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            img_path = image_dir / f\"{image_id}{ext}\"\n",
    "            if img_path.exists():\n",
    "                image_path = str(img_path)\n",
    "                break\n",
    "        \n",
    "        if not image_path:\n",
    "            continue\n",
    "            \n",
    "        # Get diagnosis info\n",
    "        diagnosis = item['dx']\n",
    "        dx_names = {\n",
    "            'akiec': 'actinic keratosis',\n",
    "            'bcc': 'basal cell carcinoma', \n",
    "            'bkl': 'benign keratosis-like lesion',\n",
    "            'df': 'dermatofibroma',\n",
    "            'mel': 'melanoma',\n",
    "            'nv': 'melanocytic nevus',\n",
    "            'vasc': 'vascular lesion'\n",
    "        }\n",
    "        diagnosis_full = dx_names.get(diagnosis, diagnosis)\n",
    "        \n",
    "        # Create conversation with optional spatial awareness\n",
    "        use_spatial = (CONFIG[\"include_spatial_descriptions\"] and \n",
    "                      item.get('mask_available', False) and \n",
    "                      random.random() < CONFIG[\"spatial_description_ratio\"])\n",
    "        \n",
    "        if use_spatial and item.get('spatial_description'):\n",
    "            user_prompt = \"Analyze this skin lesion, provide a diagnosis, and describe its location.\"\n",
    "            spatial_desc = item['spatial_description'].replace('lesion located in', 'The lesion is located in the')\n",
    "            assistant_response = f\"This appears to be {diagnosis_full}. {spatial_desc}.\"\n",
    "        else:\n",
    "            user_prompt = \"Analyze this skin lesion and provide a diagnosis.\"\n",
    "            assistant_response = f\"This appears to be {diagnosis_full}.\"\n",
    "        \n",
    "        conversation = {\n",
    "            \"image_path\": image_path,\n",
    "            \"conversation\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": user_prompt}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": assistant_response}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"metadata\": {\n",
    "                \"lesion_id\": item.get('lesion_id'),\n",
    "                \"diagnosis\": diagnosis,\n",
    "                \"has_spatial\": use_spatial,\n",
    "                \"bbox\": item.get('bbox'),\n",
    "                \"area_coverage\": item.get('area_coverage'),\n",
    "                \"mask_available\": item.get('mask_available', False)\n",
    "            }\n",
    "        }\n",
    "        conversations.append(conversation)\n",
    "        \n",
    "        if limit and len(conversations) >= limit:\n",
    "            break\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "dataset = load_spatial_dataset(limit=1000)\n",
    "spatial_count = sum(1 for c in dataset if c['metadata']['has_spatial'])\n",
    "print(f\"Dataset loaded ✓ ({len(dataset)} samples, {spatial_count} with spatial descriptions)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enhanced data collator for spatial-aware dataset\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Handle both local files and HF dataset images\n",
    "        if example[\"image_path\"] is None:\n",
    "            # HF dataset: image is already loaded in conversation\n",
    "            for msg in example[\"conversation\"]:\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    for content in msg[\"content\"]:\n",
    "                        if content[\"type\"] == \"image\":\n",
    "                            image = content[\"image\"].convert('RGB')\n",
    "                            break\n",
    "        else:\n",
    "            # Local dataset: load from file path\n",
    "            image = Image.open(example[\"image_path\"]).convert('RGB')\n",
    "        \n",
    "        images.append(image)\n",
    "        \n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"conversation\"], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        images=images, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_length\"]\n",
    "    )\n",
    "    \n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Mask image tokens in labels to prevent learning image embeddings as text\n",
    "    if hasattr(processor, 'image_token_id'):\n",
    "        labels[labels == processor.image_token_id] = -100\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare datasets with proper train/test split\n",
    "import random\n",
    "random.seed(CONFIG[\"seed\"])\n",
    "\n",
    "# Shuffle dataset to avoid bias\n",
    "shuffled_dataset = dataset.copy()\n",
    "random.shuffle(shuffled_dataset)\n",
    "\n",
    "# Use 70/30 split for better evaluation\n",
    "train_size = int(0.8 * len(shuffled_dataset))\n",
    "train_data = shuffled_dataset[:train_size]\n",
    "eval_data = shuffled_dataset[train_size:]\n",
    "\n",
    "train_dataset = HFDataset.from_list(train_data)\n",
    "eval_dataset = HFDataset.from_list(eval_data)\n",
    "\n",
    "print(f\"Training: {len(train_data)} | Evaluation: {len(eval_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    \n",
    "    # Learning rate scheduling and regularization\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    max_grad_norm=CONFIG[\"max_grad_norm\"],\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=50,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=1,\n",
    "    \n",
    "    # Memory and performance optimizations\n",
    "    remove_unused_columns=False,\n",
    "    bf16=True,\n",
    "    # gradient_checkpointing=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete ✓\")\n",
    "\n",
    "# Save model and processor\n",
    "trainer.save_model(CONFIG[\"output_dir\"])\n",
    "processor.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model inference\n",
    "\n",
    "def test_model_inference(model, processor, test_samples, num_tests=50):\n",
    "    \"\"\"Test model inference on sample images\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, sample in enumerate(test_samples[:num_tests]):\n",
    "        print(f\"\\n--- Test {i+1}/{num_tests} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Load test image\n",
    "            image_path = sample[\"image_path\"]\n",
    "            test_image = Image.open(image_path).convert('RGB')\n",
    "            print(f\"Image: {os.path.basename(image_path)}\")\n",
    "            \n",
    "            # Create conversation for inference (test both diagnosis and spatial)\n",
    "            has_spatial_data = sample[\"metadata\"][\"has_spatial\"]\n",
    "            if has_spatial_data:\n",
    "                user_prompt = \"Analyze this skin lesion, provide a diagnosis, and describe its location.\"\n",
    "            else:\n",
    "                user_prompt = \"Analyze this skin lesion and provide a diagnosis.\"\n",
    "                \n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": user_prompt}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Process input\n",
    "            text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "            inputs = processor(\n",
    "                text=[text_prompt], \n",
    "                images=[test_image], \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=processor.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            response = processor.batch_decode(\n",
    "                generated_ids[:, inputs[\"input_ids\"].shape[1]:], \n",
    "                skip_special_tokens=True\n",
    "            )[0].strip()\n",
    "            \n",
    "            # Get ground truth from dataset\n",
    "            ground_truth = sample[\"conversation\"][1][\"content\"][0][\"text\"]\n",
    "            \n",
    "            # Simple and minimal evaluation\n",
    "            bbox_gt = sample[\"metadata\"].get(\"bbox\")\n",
    "            \n",
    "            # Debug: Print what's in metadata\n",
    "            if i == 0:  # Only for first test\n",
    "                print(f\"DEBUG - Metadata keys: {list(sample['metadata'].keys())}\")\n",
    "                print(f\"DEBUG - Has bbox: {bbox_gt}\")\n",
    "                print(f\"DEBUG - Has spatial: {sample['metadata'].get('has_spatial')}\")\n",
    "            \n",
    "            # Extract diagnosis keywords for comparison\n",
    "            diagnosis_keywords = {\n",
    "                'actinic keratosis': 'akiec',\n",
    "                'basal cell carcinoma': 'bcc', \n",
    "                'benign keratosis-like lesion': 'bkl',\n",
    "                'dermatofibroma': 'df',\n",
    "                'melanoma': 'mel',\n",
    "                'melanocytic nevus': 'nv',\n",
    "                'vascular lesion': 'vasc'\n",
    "            }\n",
    "            \n",
    "            # Find diagnosis in ground truth and prediction\n",
    "            gt_dx = None\n",
    "            pred_dx = None\n",
    "            \n",
    "            for full_name, short_code in diagnosis_keywords.items():\n",
    "                if full_name in ground_truth.lower():\n",
    "                    gt_dx = short_code\n",
    "                if full_name in response.lower():\n",
    "                    pred_dx = short_code\n",
    "                    \n",
    "            diagnosis_correct = gt_dx == pred_dx and gt_dx is not None\n",
    "            \n",
    "            # Check spatial if available\n",
    "            spatial_correct = True  # Default\n",
    "            gt_spatial = \"\"\n",
    "            pred_spatial = \"\"\n",
    "            \n",
    "            if has_spatial_data:\n",
    "                if \"located in\" in ground_truth.lower():\n",
    "                    gt_spatial = ground_truth.split(\"located in\")[-1].strip().rstrip(\".\")\n",
    "                if \"located in\" in response.lower():\n",
    "                    pred_spatial = response.split(\"located in\")[-1].strip().rstrip(\".\")\n",
    "                \n",
    "                if gt_spatial and pred_spatial:\n",
    "                    spatial_correct = gt_spatial.lower() == pred_spatial.lower()\n",
    "                elif gt_spatial and not pred_spatial:\n",
    "                    spatial_correct = False  # Should have predicted spatial\n",
    "                elif not gt_spatial and pred_spatial:\n",
    "                    spatial_correct = False  # Shouldn't have predicted spatial\n",
    "            \n",
    "            is_correct = diagnosis_correct and spatial_correct\n",
    "            \n",
    "            print(f\"Ground Truth: {ground_truth}\")\n",
    "            print(f\"Model Output: {response}\")\n",
    "            if has_spatial_data and (gt_spatial or pred_spatial):\n",
    "                print(f\"Spatial GT: '{gt_spatial}' | Pred: '{pred_spatial}'\")\n",
    "                if bbox_gt and len(bbox_gt) == 4:\n",
    "                    print(f\"Bbox: [{bbox_gt[0]:.0f}, {bbox_gt[1]:.0f}, {bbox_gt[2]:.0f}, {bbox_gt[3]:.0f}]\")\n",
    "                    area_cov = sample[\"metadata\"].get(\"area_coverage\")\n",
    "                    if area_cov:\n",
    "                        print(f\"Area Coverage: {area_cov:.2%}\")\n",
    "                elif sample[\"metadata\"].get(\"mask_available\"):\n",
    "                    print(f\"Bbox: Available but not loaded properly\")\n",
    "                else:\n",
    "                    print(f\"Bbox: Not available for this sample\")\n",
    "            print(f\"Diagnosis: {'✓' if diagnosis_correct else '❌'} | Spatial: {'✓' if spatial_correct else '❌' if has_spatial_data else 'N/A'}\")\n",
    "            print(f\"Overall: {'✓ CORRECT' if is_correct else '❌ INCORRECT'}\")\n",
    "            \n",
    "            results.append({\n",
    "                \"image\": os.path.basename(image_path),\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"prediction\": response,\n",
    "                \"diagnosis_correct\": diagnosis_correct,\n",
    "                \"spatial_correct\": spatial_correct,\n",
    "                \"has_spatial\": has_spatial_data,\n",
    "                \"bbox\": bbox_gt,\n",
    "                \"success\": is_correct\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing test {i+1}: {e}\")\n",
    "            results.append({\n",
    "                \"image\": os.path.basename(image_path) if 'image_path' in locals() else f\"test_{i+1}\",\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Loading model and processor and Run inference tests\n",
    "print(\"Running inference tests on trained model...\")\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(CONFIG[\"output_dir\"],\n",
    "                             device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
    "                             trust_remote_code=True)\n",
    "processor = Qwen2VLProcessor.from_pretrained(CONFIG[\"output_dir\"],\n",
    "                                             trust_remote_code=True)\n",
    "\n",
    "# Load test dataset directly from image folder\n",
    "test_dataset = load_spatial_dataset()\n",
    "test_results = test_model_inference(model, processor, test_dataset, num_tests=50)\n",
    "\n",
    "# Enhanced summary with diagnosis and spatial accuracy\n",
    "successful_tests = sum(1 for r in test_results if r[\"success\"])\n",
    "diagnosis_correct = sum(1 for r in test_results if r[\"diagnosis_correct\"])\n",
    "spatial_tests = [r for r in test_results if r[\"has_spatial\"]]\n",
    "spatial_correct = sum(1 for r in spatial_tests if r[\"spatial_correct\"])\n",
    "\n",
    "total_tests = len(test_results)\n",
    "overall_accuracy = (successful_tests / total_tests) * 100 if total_tests > 0 else 0\n",
    "diagnosis_accuracy = (diagnosis_correct / total_tests) * 100 if total_tests > 0 else 0\n",
    "spatial_accuracy = (spatial_correct / len(spatial_tests)) * 100 if spatial_tests else 0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ENHANCED TEST SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Overall Accuracy:    {successful_tests}/{total_tests} ({overall_accuracy:.1f}%)\")\n",
    "print(f\"Diagnosis Accuracy:  {diagnosis_correct}/{total_tests} ({diagnosis_accuracy:.1f}%)\")\n",
    "print(f\"Spatial Accuracy:    {spatial_correct}/{len(spatial_tests)} ({spatial_accuracy:.1f}%) [{len(spatial_tests)} spatial samples]\")\n",
    "\n",
    "# Breakdown of failures\n",
    "diagnosis_fails = [r for r in test_results if not r[\"diagnosis_correct\"]]\n",
    "spatial_fails = [r for r in spatial_tests if not r[\"spatial_correct\"]]\n",
    "\n",
    "print(f\"\\nFailure Breakdown:\")\n",
    "print(f\"• Diagnosis errors: {len(diagnosis_fails)}\")\n",
    "print(f\"• Spatial errors: {len(spatial_fails)}\")\n",
    "\n",
    "# Show diagnosis failures\n",
    "if diagnosis_fails:\n",
    "    print(f\"\\nDiagnosis Failures ({len(diagnosis_fails)} total):\")\n",
    "    for i, fail in enumerate(diagnosis_fails[:3]):\n",
    "        gt_diag = fail['ground_truth'].split('.')[0].strip()\n",
    "        pred_diag = fail['prediction'].split('.')[0].strip()\n",
    "        print(f\"  {fail['image']}: {gt_diag} → {pred_diag}\")\n",
    "    if len(diagnosis_fails) > 3:\n",
    "        print(f\"  ... and {len(diagnosis_fails) - 3} more\")\n",
    "\n",
    "# Show spatial failures  \n",
    "if spatial_fails:\n",
    "    print(f\"\\nSpatial Failures ({len(spatial_fails)} total):\")\n",
    "    for i, fail in enumerate(spatial_fails[:3]):\n",
    "        print(f\"  {fail['image']}: GT vs Pred spatial mismatch\")\n",
    "    if len(spatial_fails) > 3:\n",
    "        print(f\"  ... and {len(spatial_fails) - 3} more\")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8074728,
     "sourceId": 12772711,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
