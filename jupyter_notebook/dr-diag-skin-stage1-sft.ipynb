{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen 2.5-2B VL Training on HAM10000 Dataset\n",
    "\n",
    "**Streamlined training pipeline for Kaggle environment**\n",
    "\n",
    "Features: QLoRA + 4-bit quantization for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install transformers accelerate peft tqdm pillow pandas bitsandbytes --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import libraries and configuration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    Qwen2VLProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset as HFDataset\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    \"train_image_dir\": \"/kaggle/input/small-isic\",\n",
    "    \"train_metadata_file\": \"/kaggle/input/small-isic/HAM10000_metadata.csv\",\n",
    "    \"output_dir\": \"./qwen2_5_vl_trained\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"max_length\": 512,\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Additional stability settings\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "# Apply QLoRA with gradient fix\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "#     lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "#     r=CONFIG[\"lora_r\"],\n",
    "#     bias=\"none\",\n",
    "#     target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "#     task_type=TaskType.CAUSAL_LM,\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "# Fix gradient computation\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'lora' in name:\n",
    "#         param.requires_grad = True\n",
    "\n",
    "print(\"Model loaded ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load HAM10000 dataset\n",
    "def load_ham10000_data():\n",
    "    image_dir = Path(CONFIG[\"train_image_dir\"])\n",
    "    metadata_file = CONFIG[\"train_metadata_file\"]\n",
    "    df = pd.read_csv(metadata_file)\n",
    "    \n",
    "    # Find image files\n",
    "    image_files = []\n",
    "    for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\"]:\n",
    "        image_files.extend(list(image_dir.glob(f\"**/{ext}\")))\n",
    "    \n",
    "    conversations = []\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = str(row.get('image_id', row.get('Image', '')))\n",
    "        matching_images = [img for img in image_files if image_id in str(img)]\n",
    "        \n",
    "        if matching_images:\n",
    "            diagnosis = str(row.get('dx', row.get('diagnosis', 'skin lesion')))\n",
    "            conversation = {\n",
    "                \"image_path\": str(matching_images[0]),\n",
    "                \"conversation\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"image\"},\n",
    "                            {\"type\": \"text\", \"text\": \"Analyze this skin lesion and provide a diagnosis.\"}\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": f\"This appears to be {diagnosis}.\"}\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            conversations.append(conversation)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "dataset = load_ham10000_data()\n",
    "print(f\"Dataset loaded ✓ ({len(dataset)} samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data collator\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        image = Image.open(example[\"image_path\"]).convert('RGB')\n",
    "        images.append(image)\n",
    "        \n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"conversation\"], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        images=images, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_length\"]\n",
    "    )\n",
    "    \n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Mask image tokens in labels to prevent learning image embeddings as text\n",
    "    if hasattr(processor, 'image_token_id'):\n",
    "        labels[labels == processor.image_token_id] = -100\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_data = dataset[:train_size]\n",
    "eval_data = dataset[train_size:]\n",
    "\n",
    "train_dataset = HFDataset.from_list(train_data)\n",
    "eval_dataset = HFDataset.from_list(eval_data)\n",
    "\n",
    "print(f\"Training: {len(train_data)} | Evaluation: {len(eval_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    \n",
    "    # Learning rate scheduling and regularization\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    max_grad_norm=CONFIG[\"max_grad_norm\"],\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,  # Less frequent saves\n",
    "    eval_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,  # Keep only 2 checkpoints\n",
    "    \n",
    "    # Memory and performance optimizations\n",
    "    remove_unused_columns=False,\n",
    "    bf16=True,\n",
    "    # gradient_checkpointing=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete ✓\")\n",
    "\n",
    "# Save model and processor\n",
    "trainer.save_model(CONFIG[\"output_dir\"])\n",
    "processor.save_pretrained(CONFIG[\"output_dir\"])\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model inference\n",
    "\n",
    "def test_model_inference(model, processor, test_samples, num_tests=3):\n",
    "    \"\"\"Test model inference on sample images\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, sample in enumerate(test_samples[:num_tests]):\n",
    "        print(f\"\\n--- Test {i+1}/{num_tests} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Load test image\n",
    "            image_path = sample[\"image_path\"]\n",
    "            test_image = Image.open(image_path).convert('RGB')\n",
    "            print(f\"Image: {os.path.basename(image_path)}\")\n",
    "            \n",
    "            # Create conversation for inference\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": \"Analyze this skin lesion and provide a diagnosis.\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Process input\n",
    "            text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "            inputs = processor(\n",
    "                text=[text_prompt], \n",
    "                images=[test_image], \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=processor.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            response = processor.batch_decode(\n",
    "                generated_ids[:, inputs[\"input_ids\"].shape[1]:], \n",
    "                skip_special_tokens=True\n",
    "            )[0].strip()\n",
    "            \n",
    "            # Get ground truth from dataset\n",
    "            ground_truth = sample[\"conversation\"][1][\"content\"][0][\"text\"]\n",
    "            \n",
    "            print(f\"Ground Truth: {ground_truth}\")\n",
    "            print(f\"Model Output: {response}\")\n",
    "            \n",
    "            results.append({\n",
    "                \"image\": os.path.basename(image_path),\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"prediction\": response,\n",
    "                \"success\": True\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing test {i+1}: {e}\")\n",
    "            results.append({\n",
    "                \"image\": os.path.basename(image_path) if 'image_path' in locals() else f\"test_{i+1}\",\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Loading model and processor and Run inference tests\n",
    "print(\"Running inference tests on trained model...\")\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(CONFIG[\"output_dir\"],\n",
    "                             device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
    "                             trust_remote_code=True)\n",
    "processor = Qwen2VLProcessor.from_pretrained(CONFIG[\"output_dir\"],\n",
    "                                             trust_remote_code=True)\n",
    "\n",
    "test_results = test_model_inference(model, processor, dataset, num_tests=3)\n",
    "\n",
    "# Summary\n",
    "successful_tests = sum(1 for r in test_results if r[\"success\"])\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TEST SUMMARY: {successful_tests}/{len(test_results)} tests passed\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8074728,
     "sourceId": 12772711,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
