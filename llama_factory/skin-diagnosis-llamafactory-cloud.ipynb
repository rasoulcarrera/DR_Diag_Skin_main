{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skin Disease Diagnosis - LLaMA Factory (Cloud)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check system and install dependencies\n",
        "import torch\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Check hardware\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    print(f\"GPU count: {gpu_count}\")\n",
        "    for i in range(gpu_count):\n",
        "        gpu_name = torch.cuda.get_device_name(i)\n",
        "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
        "        print(f\"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
        "\n",
        "# Install LLaMA Factory\n",
        "%pip install llamafactory[torch,metrics]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environment for 2x RTX 4090\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# Configure for dual RTX 4090 setup\n",
        "gpu_count = torch.cuda.device_count()\n",
        "if gpu_count >= 2:\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
        "    print(f\"Configured for {gpu_count} GPUs\")\n",
        "else:\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "    print(f\"Single GPU setup: {gpu_count}\")\n",
        "\n",
        "# Disable wandb if not needed\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and prepare ISIC dataset\n",
        "# For cloud machines, we'll download the dataset directly\n",
        "\n",
        "import wget\n",
        "import zipfile\n",
        "\n",
        "# Download ISIC 2018 dataset (or your preferred dataset)\n",
        "dataset_url = \"https://challenge.isic-archive.com/data/\"  # Update with actual URL\n",
        "data_dir = \"/workspace/skin_data\"  # Common path for cloud instances\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "print(\"Dataset directory created. Please upload your ISIC dataset to:\", data_dir)\n",
        "print(\"Expected structure:\")\n",
        "print(\"  /workspace/skin_data/images/\")\n",
        "print(\"  /workspace/skin_data/metadata.csv\")\n",
        "\n",
        "# Alternative: Use a smaller test dataset\n",
        "# You can modify this to download from your preferred source\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset for LLaMA Factory\n",
        "def prepare_skin_dataset(image_dir, metadata_file, output_dir):\n",
        "    \"\"\"Convert skin dataset to LLaMA Factory format\"\"\"\n",
        "    \n",
        "    # Load metadata\n",
        "    if metadata_file.endswith('.json'):\n",
        "        with open(metadata_file, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "    else:\n",
        "        df = pd.read_csv(metadata_file)\n",
        "        metadata = df.to_dict('records')\n",
        "    \n",
        "    llamafactory_data = []\n",
        "    \n",
        "    for item in metadata:\n",
        "        # Handle different column names\n",
        "        image_cols = ['image_name', 'isic_id', 'image_id', 'filename']\n",
        "        image_filename = None\n",
        "        for col in image_cols:\n",
        "            if col in item and item[col]:\n",
        "                image_filename = str(item[col])\n",
        "                break\n",
        "        \n",
        "        if not image_filename:\n",
        "            continue\n",
        "            \n",
        "        # Add extension if missing\n",
        "        if not image_filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            image_filename += '.jpg'\n",
        "        \n",
        "        image_path = os.path.join(image_dir, image_filename)\n",
        "        \n",
        "        if os.path.exists(image_path):\n",
        "            # Build response\n",
        "            diagnosis = item.get('diagnosis', item.get('dx', 'skin lesion'))\n",
        "            response = f\"This image shows {diagnosis}.\"\n",
        "            \n",
        "            # Add location info if available\n",
        "            if 'localization' in item:\n",
        "                response += f\" Located on the {item['localization']}.\"\n",
        "            elif 'location' in item:\n",
        "                response += f\" Located on the {item['location']}.\"\n",
        "            \n",
        "            # Add metadata\n",
        "            if 'age' in item:\n",
        "                response += f\" Patient age: {item['age']}.\"\n",
        "            if 'sex' in item:\n",
        "                response += f\" Patient gender: {item['sex']}.\"\n",
        "            \n",
        "            entry = {\n",
        "                \"conversations\": [\n",
        "                    {\n",
        "                        \"from\": \"human\", \n",
        "                        \"value\": \"<image>\\\\nAnalyze this skin lesion and provide a diagnosis.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"from\": \"gpt\",\n",
        "                        \"value\": response\n",
        "                    }\n",
        "                ],\n",
        "                \"images\": [image_filename]\n",
        "            }\n",
        "            \n",
        "            llamafactory_data.append(entry)\n",
        "    \n",
        "    # Save dataset\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    dataset_file = os.path.join(output_dir, 'skin_dataset.json')\n",
        "    with open(dataset_file, 'w') as f:\n",
        "        json.dump(llamafactory_data, f, indent=2)\n",
        "    \n",
        "    return dataset_file, len(llamafactory_data)\n",
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    \"image_dir\": \"/workspace/skin_data/images\",\n",
        "    \"metadata_file\": \"/workspace/skin_data/metadata.csv\", \n",
        "    \"output_dir\": \"/workspace/llamafactory_data\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert dataset (run this after uploading your data)\n",
        "if os.path.exists(config[\"metadata_file\"]):\n",
        "    dataset_file, num_samples = prepare_skin_dataset(\n",
        "        image_dir=config[\"image_dir\"],\n",
        "        metadata_file=config[\"metadata_file\"], \n",
        "        output_dir=config[\"output_dir\"]\n",
        "    )\n",
        "    print(f\"Dataset prepared: {num_samples} samples\")\n",
        "    print(f\"Saved to: {dataset_file}\")\n",
        "else:\n",
        "    print(\"Metadata file not found. Please upload your dataset first.\")\n",
        "    print(f\"Looking for: {config['metadata_file']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset configuration for LLaMA Factory\n",
        "dataset_info = {\n",
        "    \"skin_diagnosis\": {\n",
        "        \"file_name\": \"skin_dataset.json\",\n",
        "        \"formatting\": \"sharegpt\",\n",
        "        \"columns\": {\n",
        "            \"messages\": \"conversations\",\n",
        "            \"images\": \"images\"\n",
        "        },\n",
        "        \"tags\": {\n",
        "            \"role_tag\": \"from\",\n",
        "            \"content_tag\": \"value\",\n",
        "            \"user_tag\": \"human\", \n",
        "            \"assistant_tag\": \"gpt\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save dataset config\n",
        "dataset_config_file = os.path.join(config[\"output_dir\"], 'dataset_info.json')\n",
        "with open(dataset_config_file, 'w') as f:\n",
        "    json.dump(dataset_info, f, indent=2)\n",
        "\n",
        "print(f\"Dataset config saved: {dataset_config_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimized training script for 2x RTX 4090\n",
        "def create_cloud_training_script(data_dir):\n",
        "    \"\"\"Create training script optimized for dual RTX 4090\"\"\"\n",
        "    \n",
        "    training_script = f\"\"\"\n",
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "\n",
        "# Environment setup\n",
        "gpu_count = torch.cuda.device_count()\n",
        "print(f\"Detected {{gpu_count}} GPUs\")\n",
        "\n",
        "# RTX 4090 optimized settings\n",
        "if gpu_count >= 2:\n",
        "    # Dual RTX 4090 setup (48GB total VRAM)\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
        "    per_device_batch_size = \"4\"  # Higher batch size for RTX 4090\n",
        "    gradient_accumulation = \"2\"   # Lower accumulation needed\n",
        "    max_samples = \"2000\"         # More samples\n",
        "    epochs = \"5\"                 # More epochs\n",
        "    dataloader_workers = \"8\"     # More workers for faster loading\n",
        "else:\n",
        "    # Single GPU fallback\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "    per_device_batch_size = \"2\"\n",
        "    gradient_accumulation = \"4\"\n",
        "    max_samples = \"1000\"\n",
        "    epochs = \"3\"\n",
        "    dataloader_workers = \"4\"\n",
        "\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "# Training command\n",
        "cmd = [\n",
        "    \"llamafactory-cli\", \"train\",\n",
        "    \"--stage\", \"sft\",\n",
        "    \"--do_train\",\n",
        "    \"--model_name_or_path\", \"Qwen/Qwen2-VL-2B-Instruct\",\n",
        "    \"--dataset\", \"skin_diagnosis\",\n",
        "    \"--dataset_dir\", \"{data_dir}\",\n",
        "    \"--template\", \"qwen2_vl\", \n",
        "    \"--finetuning_type\", \"lora\",\n",
        "    \"--lora_target\", \"all\",\n",
        "    \"--lora_r\", \"64\",              # Higher rank for RTX 4090\n",
        "    \"--lora_alpha\", \"128\",\n",
        "    \"--output_dir\", \"/workspace/skin_model_output\",\n",
        "    \"--overwrite_output_dir\",\n",
        "    \"--per_device_train_batch_size\", per_device_batch_size,\n",
        "    \"--gradient_accumulation_steps\", gradient_accumulation,\n",
        "    \"--lr_scheduler_type\", \"cosine\",\n",
        "    \"--logging_steps\", \"10\",\n",
        "    \"--warmup_ratio\", \"0.1\",\n",
        "    \"--save_steps\", \"200\",\n",
        "    \"--learning_rate\", \"2e-4\",     # Higher LR for faster training\n",
        "    \"--num_train_epochs\", epochs,\n",
        "    \"--max_samples\", max_samples,\n",
        "    \"--val_size\", \"0.1\",\n",
        "    \"--evaluation_strategy\", \"steps\",\n",
        "    \"--eval_steps\", \"200\",\n",
        "    \"--plot_loss\",\n",
        "    \"--fp16\",\n",
        "    \"--visual_inputs\",\n",
        "    \"--freeze_vision_tower\",\n",
        "    \"--dataloader_num_workers\", dataloader_workers,\n",
        "    \"--save_total_limit\", \"5\",\n",
        "    \"--load_best_model_at_end\",\n",
        "    \"--metric_for_best_model\", \"eval_loss\"\n",
        "]\n",
        "\n",
        "# Multi-GPU optimizations\n",
        "if gpu_count >= 2:\n",
        "    cmd.extend([\n",
        "        \"--ddp_find_unused_parameters\", \"false\",\n",
        "        \"--dataloader_pin_memory\", \"true\",\n",
        "        \"--group_by_length\", \"false\"\n",
        "    ])\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  GPUs: {{gpu_count}}\")\n",
        "print(f\"  Batch size per device: {{per_device_batch_size}}\")\n",
        "print(f\"  Gradient accumulation: {{gradient_accumulation}}\")\n",
        "print(f\"  Max samples: {{max_samples}}\")\n",
        "print(f\"  Epochs: {{epochs}}\")\n",
        "\n",
        "result = subprocess.run(cmd, check=True)\n",
        "print(\"Training completed successfully!\")\n",
        "\"\"\"\n",
        "    \n",
        "    script_path = \"/workspace/train_skin_model.py\"\n",
        "    with open(script_path, 'w') as f:\n",
        "        f.write(training_script)\n",
        "    \n",
        "    return script_path\n",
        "\n",
        "# Create training script\n",
        "train_script = create_cloud_training_script(config[\"output_dir\"])\n",
        "print(f\"Training script created: {train_script}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training process...\")\n",
        "print(\"This will take some time depending on your dataset size\")\n",
        "print(\"For 2x RTX 4090: expect ~1-2 hours for 2000 samples\")\n",
        "\n",
        "exec(open('/workspace/train_skin_model.py').read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model\n",
        "def create_inference_script():\n",
        "    \"\"\"Create script to test the trained model\"\"\"\n",
        "    \n",
        "    inference_script = \"\"\"\n",
        "#!/bin/bash\n",
        "\n",
        "echo \"Testing the trained skin diagnosis model...\"\n",
        "\n",
        "llamafactory-cli chat \\\\\n",
        "    --model_name_or_path Qwen/Qwen2-VL-2B-Instruct \\\\\n",
        "    --adapter_name_or_path /workspace/skin_model_output \\\\\n",
        "    --template qwen2_vl \\\\\n",
        "    --finetuning_type lora\n",
        "\"\"\"\n",
        "    \n",
        "    with open(\"/workspace/test_model.sh\", 'w') as f:\n",
        "        f.write(inference_script)\n",
        "    \n",
        "    os.chmod(\"/workspace/test_model.sh\", 0o755)\n",
        "    print(\"Inference script created: /workspace/test_model.sh\")\n",
        "\n",
        "# Create web interface script\n",
        "def create_webui_script():\n",
        "    \"\"\"Create script for LLaMA Factory web interface\"\"\"\n",
        "    \n",
        "    webui_script = \"\"\"\n",
        "#!/bin/bash\n",
        "\n",
        "echo \"Starting LLaMA Factory Web Interface...\"\n",
        "\n",
        "llamafactory-cli webui \\\\\n",
        "    --host 0.0.0.0 \\\\\n",
        "    --port 7860\n",
        "\"\"\"\n",
        "    \n",
        "    with open(\"/workspace/launch_webui.sh\", 'w') as f:\n",
        "        f.write(webui_script)\n",
        "    \n",
        "    os.chmod(\"/workspace/launch_webui.sh\", 0o755)\n",
        "    print(\"Web UI script created: /workspace/launch_webui.sh\")\n",
        "\n",
        "# Create scripts\n",
        "create_inference_script()\n",
        "create_webui_script()\n",
        "\n",
        "print(\"\\\\nModel testing options:\")\n",
        "print(\"1. CLI chat: bash /workspace/test_model.sh\")\n",
        "print(\"2. Web interface: bash /workspace/launch_webui.sh\")\n",
        "print(\"3. Output model: /workspace/skin_model_output\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cloud Setup Instructions\n",
        "\n",
        "### Vast.ai Setup:\n",
        "1. **Choose instance**: Search for \"RTX 4090\" with 2+ GPUs\n",
        "2. **Select template**: PyTorch or Deep Learning template\n",
        "3. **Connect**: Use Jupyter or SSH\n",
        "4. **Upload data**: Upload your ISIC dataset to `/workspace/skin_data/`\n",
        "\n",
        "### Expected Performance (2x RTX 4090):\n",
        "- **VRAM**: 48GB total (24GB per GPU)\n",
        "- **Training speed**: ~2x faster than single GPU\n",
        "- **Batch size**: 4 per GPU = 8 total\n",
        "- **Training time**: ~1-2 hours for 2000 samples\n",
        "\n",
        "### Commands:\n",
        "- **Training**: `python /workspace/train_skin_model.py`\n",
        "- **Testing**: `bash /workspace/test_model.sh`\n",
        "- **Web UI**: `bash /workspace/launch_webui.sh`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
