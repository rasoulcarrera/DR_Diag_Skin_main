{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Disease Diagnosis - Stage 1 SFT Training\n",
    "\n",
    "Simple and clean implementation using direct processor approach for Vision-Language Model training.\n",
    "\n",
    "## Overview\n",
    "- **Stage 1 Goal**: Disease identification with basic spatial awareness\n",
    "- **Model**: Qwen2-VL-2B-Instruct with LoRA fine-tuning\n",
    "- **Dataset**: ISIC skin lesion dataset with metadata\n",
    "- **Output**: Foundation model for Stage 2 GRPO training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install transformers[torch] accelerate peft tqdm pillow pandas\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:45:42.046777Z",
     "iopub.status.busy": "2025-08-15T14:45:42.046473Z",
     "iopub.status.idle": "2025-08-15T14:45:50.557680Z",
     "shell.execute_reply": "2025-08-15T14:45:50.556841Z",
     "shell.execute_reply.started": "2025-08-15T14:45:42.046756Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 14:45:47.714459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755269147.736753     838 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755269147.743644     838 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import (\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    AutoTokenizer, \n",
    "    AutoProcessor,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:45:51.154649Z",
     "iopub.status.busy": "2025-08-15T14:45:51.154381Z",
     "iopub.status.idle": "2025-08-15T14:45:51.160932Z",
     "shell.execute_reply": "2025-08-15T14:45:51.160298Z",
     "shell.execute_reply.started": "2025-08-15T14:45:51.154631Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n"
     ]
    }
   ],
   "source": [
    "# Simple configuration\n",
    "config = {\n",
    "    \"model_name\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    \"output_dir\": \"/kaggle/working/outputs\",\n",
    "    \"train_image_dir\": \"/kaggle/input/small-isic\",\n",
    "    \"train_metadata_file\": \"/kaggle/input/small-isic/HAM10000_metadata.csv\",\n",
    "    \n",
    "    \"training\": {\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 8,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"max_length\": 512,\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \n",
    "    \"lora\": {\n",
    "        \"r\": 16,\n",
    "        \"alpha\": 32,\n",
    "        \"dropout\": 0.1,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs(config['output_dir'], exist_ok=True)\n",
    "set_seed(config['training']['seed'])\n",
    "print(\"Configuration loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing Class (Based on Your Approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:45:53.988682Z",
     "iopub.status.busy": "2025-08-15T14:45:53.987894Z",
     "iopub.status.idle": "2025-08-15T14:45:54.000059Z",
     "shell.execute_reply": "2025-08-15T14:45:53.999117Z",
     "shell.execute_reply.started": "2025-08-15T14:45:53.988653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingClass defined!\n"
     ]
    }
   ],
   "source": [
    "class ProcessingClass:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.tokenizer = processor.tokenizer\n",
    "    \n",
    "    def __call__(self, examples=None, text=None, **kwargs):\n",
    "        if examples is not None:\n",
    "            images = examples.get(\"image\", [])\n",
    "            prompts_raw = examples.get(\"prompt\", [])\n",
    "            answers = examples.get(\"answer\", [])\n",
    "\n",
    "            # Process prompts - they should already contain image tokens from dataset\n",
    "            processed_prompts = []\n",
    "            for prompt_turns in prompts_raw:\n",
    "                if isinstance(prompt_turns, str):\n",
    "                    # Use the prompt as-is (should already have image tokens)\n",
    "                    processed_prompts.append(prompt_turns)\n",
    "                else:\n",
    "                    # Convert to string if not already\n",
    "                    processed_prompts.append(str(prompt_turns))\n",
    "\n",
    "            # Process images\n",
    "            processed_images = []\n",
    "            for img in images:\n",
    "                if isinstance(img, bytes):\n",
    "                    try:\n",
    "                        processed_images.append(Image.open(BytesIO(img)).convert(\"RGB\"))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading image from bytes: {e}\")\n",
    "                        processed_images.append(Image.new(\"RGB\", (224, 224)))\n",
    "                elif isinstance(img, Image.Image):\n",
    "                    processed_images.append(img.convert(\"RGB\"))\n",
    "                else:\n",
    "                    print(f\"Warning: Image format not recognized: {type(img)}\")\n",
    "                    processed_images.append(Image.new(\"RGB\", (224, 224)))\n",
    "\n",
    "            max_prompt_length = kwargs.get(\"max_prompt_length\", 512)\n",
    "            max_completion_length = kwargs.get(\"max_completion_length\", 512)\n",
    "            \n",
    "            # Use processor to handle both images and text\n",
    "            inputs = self.processor(\n",
    "                images=processed_images,\n",
    "                text=processed_prompts,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=max_prompt_length\n",
    "            )\n",
    "            \n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            pixel_values = inputs.get(\"pixel_values\")\n",
    "            image_grid_thw = inputs.get(\"image_grid_thw\")\n",
    "\n",
    "            # Process answers/labels\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                label_encodings = self.tokenizer(\n",
    "                    answers,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=max_completion_length\n",
    "                )\n",
    "            labels_tensor = label_encodings[\"input_ids\"].to(input_ids.device)\n",
    "            labels_tensor[labels_tensor == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "            result = {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"labels\": labels_tensor,\n",
    "            }\n",
    "            \n",
    "            # Add image-related tensors if they exist\n",
    "            if pixel_values is not None:\n",
    "                result[\"pixel_values\"] = pixel_values\n",
    "            if image_grid_thw is not None:\n",
    "                result[\"image_grid_thw\"] = image_grid_thw\n",
    "                \n",
    "            return result\n",
    "\n",
    "        elif text is not None:\n",
    "            return self.processor(text=text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        else:\n",
    "            raise ValueError(\"Either 'examples' or 'text' must be provided.\")\n",
    "\n",
    "    def batch_decode(self, tokenized_output, skip_special_tokens=True):\n",
    "        if isinstance(tokenized_output, torch.Tensor):\n",
    "            tokenized_output = tokenized_output.tolist()\n",
    "        return self.tokenizer.batch_decode(tokenized_output, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "print(\"ProcessingClass defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:45:57.262401Z",
     "iopub.status.busy": "2025-08-15T14:45:57.262089Z",
     "iopub.status.idle": "2025-08-15T14:45:57.273764Z",
     "shell.execute_reply": "2025-08-15T14:45:57.273139Z",
     "shell.execute_reply.started": "2025-08-15T14:45:57.262376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkinDiseaseDataset defined!\n"
     ]
    }
   ],
   "source": [
    "class SkinDiseaseDataset(Dataset):\n",
    "    def __init__(self, image_dir, metadata_file, process_instance, max_length=512):\n",
    "        self.image_dir = image_dir\n",
    "        self.process_instance = process_instance\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_df = pd.read_csv(metadata_file)\n",
    "        self.metadata = metadata_df.to_dict('records')\n",
    "        \n",
    "        # Find image column\n",
    "        possible_cols = ['image_name', 'isic_id', 'image_id', 'filename', 'name', 'image']\n",
    "        self.image_col = None\n",
    "        for col in possible_cols:\n",
    "            if col in metadata_df.columns:\n",
    "                self.image_col = col\n",
    "                break\n",
    "        if not self.image_col:\n",
    "            self.image_col = metadata_df.columns[0]\n",
    "            \n",
    "        # Prepare data\n",
    "        self.data = []\n",
    "        for item in self.metadata:\n",
    "            image_filename = str(item[self.image_col])\n",
    "            if not image_filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_filename += '.jpg'\n",
    "            \n",
    "            image_path = os.path.join(self.image_dir, image_filename)\n",
    "            if os.path.exists(image_path):\n",
    "                diagnosis = item.get('dx', 'unknown')\n",
    "                location = item.get('localization', 'body')\n",
    "                \n",
    "                prompt = \"Diagnose this skin condition.\"\n",
    "                answer = f\"{diagnosis} located on {location}\"\n",
    "                \n",
    "                self.data.append({\n",
    "                    'image_path': image_path,\n",
    "                    'prompt': prompt,\n",
    "                    'answer': answer\n",
    "                })\n",
    "        \n",
    "        print(f\"Dataset prepared with {len(self.data)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(item['image_path']).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), color='white')\n",
    "        \n",
    "        # Prepare conversation format for Qwen2VL processor\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": item['prompt']}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": item['answer']}]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Use processor's apply_chat_template\n",
    "        try:\n",
    "            text_input = self.process_instance.processor.apply_chat_template(\n",
    "                conversation, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "        except:\n",
    "            # Fallback to simple format\n",
    "            text_input = f\"<image>{item['prompt']}\"\n",
    "        \n",
    "        # Prepare examples in the format expected by ProcessingClass\n",
    "        examples = {\n",
    "            \"image\": [image],\n",
    "            \"prompt\": [text_input],\n",
    "            \"answer\": [item['answer']]\n",
    "        }\n",
    "        \n",
    "        # Process using the ProcessingClass\n",
    "        processed = self.process_instance(\n",
    "            examples=examples,\n",
    "            max_prompt_length=self.max_length,\n",
    "            max_completion_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        # Return single sample (remove batch dimension)\n",
    "        result = {}\n",
    "        for key, value in processed.items():\n",
    "            if isinstance(value, torch.Tensor) and value.dim() > 0:\n",
    "                result[key] = value.squeeze(0)  # Remove batch dimension\n",
    "            else:\n",
    "                result[key] = value\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"SkinDiseaseDataset defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:46:00.849974Z",
     "iopub.status.busy": "2025-08-15T14:46:00.849690Z",
     "iopub.status.idle": "2025-08-15T14:46:10.333189Z",
     "shell.execute_reply": "2025-08-15T14:46:10.332317Z",
     "shell.execute_reply.started": "2025-08-15T14:46:00.849954Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9c6e9bbe29465eaabe41a910142d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,358,144 || all params: 2,213,343,744 || trainable%: 0.1969\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load model, processor, and tokenizer\n",
    "print(\"Loading model and processor...\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(config['model_name'], trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'], trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    config['model_name'],\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=config['lora']['r'],\n",
    "    lora_alpha=config['lora']['alpha'],\n",
    "    lora_dropout=config['lora']['dropout'],\n",
    "    target_modules=config['lora']['target_modules']\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset and Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:47:13.813157Z",
     "iopub.status.busy": "2025-08-15T14:47:13.812195Z",
     "iopub.status.idle": "2025-08-15T14:47:14.541601Z",
     "shell.execute_reply": "2025-08-15T14:47:14.540782Z",
     "shell.execute_reply.started": "2025-08-15T14:47:13.813131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "Dataset prepared with 500 samples\n",
      "Sample shapes: {'input_ids': torch.Size([512]), 'attention_mask': torch.Size([512]), 'labels': torch.Size([512]), 'pixel_values': torch.Size([1344, 1176]), 'image_grid_thw': torch.Size([3])}\n",
      "Image token ID: None\n",
      "Could not find image token or input_ids not a tensor\n",
      "Decoded text sample (first 200 chars): <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|end\n",
      "Pixel values shape: torch.Size([1344, 1176])\n",
      "⚠️ Pixel values have wrong shape - should be [3, height, width]\n"
     ]
    }
   ],
   "source": [
    "# Create processing instance and dataset\n",
    "process_instance = ProcessingClass(processor)\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "train_dataset = SkinDiseaseDataset(\n",
    "    image_dir=config['train_image_dir'],\n",
    "    metadata_file=config['train_metadata_file'],\n",
    "    process_instance=process_instance,\n",
    "    max_length=config['training']['max_length']\n",
    ")\n",
    "\n",
    "# Test dataset and debug\n",
    "if len(train_dataset) > 0:\n",
    "    sample = train_dataset[0]\n",
    "    print(\"Sample shapes:\", {k: v.shape if hasattr(v, 'shape') else type(v) for k, v in sample.items()})\n",
    "    \n",
    "    # Debug: Check if image tokens are in the input_ids\n",
    "    input_ids = sample['input_ids']\n",
    "    try:\n",
    "        image_token_id = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "        print(f\"Image token ID: {image_token_id}\")\n",
    "        \n",
    "        if image_token_id is not None and isinstance(input_ids, torch.Tensor):\n",
    "            image_token_count = (input_ids == image_token_id).sum().item()\n",
    "            print(f\"Number of image tokens in input: {image_token_count}\")\n",
    "        else:\n",
    "            print(\"Could not find image token or input_ids not a tensor\")\n",
    "            \n",
    "        # Print decoded text to see the format\n",
    "        decoded_text = processor.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        print(f\"Decoded text sample (first 200 chars): {decoded_text[:200]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Debug error: {e}\")\n",
    "        print(f\"input_ids type: {type(input_ids)}\")\n",
    "        print(f\"input_ids shape: {input_ids.shape if hasattr(input_ids, 'shape') else 'no shape'}\")\n",
    "    \n",
    "    # Check pixel_values shape - should be [3, H, W]\n",
    "    pixel_values = sample['pixel_values']\n",
    "    print(f\"Pixel values shape: {pixel_values.shape}\")\n",
    "#     if pixel_values.shape != torch.Size([3, 224, 224]) and len(pixel_values.shape) == 2:\n",
    "#         print(\"⚠️ Pixel values have wrong shape - should be [3, height, width]\")\n",
    "# else:\n",
    "#     print(\"No samples found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:47:18.073826Z",
     "iopub.status.busy": "2025-08-15T14:47:18.073161Z",
     "iopub.status.idle": "2025-08-15T14:47:18.085302Z",
     "shell.execute_reply": "2025-08-15T14:47:18.084513Z",
     "shell.execute_reply.started": "2025-08-15T14:47:18.073800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete. Total steps: 750\n"
     ]
    }
   ],
   "source": [
    "# Setup training components\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate']\n",
    ")\n",
    "\n",
    "# Custom collate function to handle batch processing\n",
    "def collate_fn(batch):\n",
    "    # Stack all tensors\n",
    "    result = {}\n",
    "    for key in batch[0].keys():\n",
    "        values = [item[key] for item in batch]\n",
    "        if isinstance(values[0], torch.Tensor):\n",
    "            result[key] = torch.stack(values)\n",
    "        else:\n",
    "            result[key] = values\n",
    "    return result\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "total_steps = len(train_dataloader) * config['training']['num_epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(f\"Training setup complete. Total steps: {total_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:47:21.933549Z",
     "iopub.status.busy": "2025-08-15T14:47:21.933229Z",
     "iopub.status.idle": "2025-08-15T14:59:37.648276Z",
     "shell.execute_reply": "2025-08-15T14:59:37.647595Z",
     "shell.execute_reply.started": "2025-08-15T14:47:21.933530Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 0: 100%|██████████| 250/250 [04:10<00:00,  1.00s/it, Loss=4.8416] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Average Loss: 4.8416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 250/250 [04:02<00:00,  1.03it/s, Loss=0.0393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 250/250 [04:02<00:00,  1.03it/s, Loss=0.0055]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Loss: 0.0055\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        for key in batch:\n",
    "            if isinstance(batch[key], torch.Tensor):\n",
    "                batch[key] = batch[key].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update progress\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (progress_bar.n + 1)\n",
    "        progress_bar.set_postfix({'Loss': f'{avg_loss:.4f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Main training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(config['training']['num_epochs']):\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, scaler, epoch)\n",
    "    print(f\"Epoch {epoch} - Average Loss: {train_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:59:42.711913Z",
     "iopub.status.busy": "2025-08-15T14:59:42.711621Z",
     "iopub.status.idle": "2025-08-15T14:59:43.204443Z",
     "shell.execute_reply": "2025-08-15T14:59:43.203604Z",
     "shell.execute_reply.started": "2025-08-15T14:59:42.711890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final model...\n",
      "Model saved to /kaggle/working/outputs/stage1_final_model\n",
      "Training pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save final model\n",
    "print(\"Saving final model...\")\n",
    "save_path = os.path.join(config['output_dir'], 'stage1_final_model')\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Save training info\n",
    "training_info = {\n",
    "    'stage': 'stage1_sft',\n",
    "    'model_name': config['model_name'],\n",
    "    'training_config': config,\n",
    "    'description': 'Stage 1 SFT model for skin disease diagnosis'\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_path, 'training_info.json'), 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")\n",
    "print(\"Training pipeline completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_inference(model, processor, image_path, prompt=\"Diagnose this skin condition.\"):\n",
    "    \"\"\"\n",
    "    Test the trained model on a single image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and process image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        print(f\"Testing with image: {image_path}\")\n",
    "        print(f\"Image size: {image.size}\")\n",
    "        \n",
    "        # Create conversation format\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Process with the trained model\n",
    "        text_input = processor.apply_chat_template(\n",
    "            conversation, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize and process\n",
    "        inputs = processor(\n",
    "            images=[image],\n",
    "            text=[text_input],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            generate_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        generated_text = processor.batch_decode(\n",
    "            generate_ids[:, inputs['input_ids'].shape[1]:], \n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        \n",
    "        print(f\"Model prediction: {generated_text}\")\n",
    "        return generated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Test function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with sample images from the dataset\n",
    "print(\"=\"*50)\n",
    "print(\"🧪 TESTING TRAINED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(train_dataset) > 0:\n",
    "    # Test with first few samples from dataset\n",
    "    num_test_samples = min(3, len(train_dataset))\n",
    "    \n",
    "    for i in range(num_test_samples):\n",
    "        print(f\"\\n🔍 Test Sample {i+1}/{num_test_samples}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Get sample data\n",
    "        sample_data = train_dataset.data[i]\n",
    "        image_path = sample_data['image_path']\n",
    "        expected_answer = sample_data['answer']\n",
    "        \n",
    "        print(f\"Expected: {expected_answer}\")\n",
    "        \n",
    "        # Test model inference\n",
    "        prediction = test_model_inference(\n",
    "            model=model,\n",
    "            processor=processor, \n",
    "            image_path=image_path,\n",
    "            prompt=\"Diagnose this skin condition.\"\n",
    "        )\n",
    "        \n",
    "        if prediction:\n",
    "            print(f\"✅ Inference successful!\")\n",
    "        else:\n",
    "            print(f\"❌ Inference failed!\")\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"❌ No dataset samples available for testing\")\n",
    "\n",
    "print(f\"\\n🎯 Testing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom image testing function - Test with your own images!\n",
    "def test_custom_image(image_path, custom_prompt=None):\n",
    "    \"\"\"\n",
    "    Test the model with a custom image path\n",
    "    Usage: test_custom_image(\"/path/to/your/image.jpg\", \"Diagnose this lesion\")\n",
    "    \"\"\"\n",
    "    if custom_prompt is None:\n",
    "        custom_prompt = \"Diagnose this skin condition and describe its location.\"\n",
    "    \n",
    "    print(f\"\\n🔬 CUSTOM IMAGE TEST\")\n",
    "    print(f\"Image: {image_path}\")\n",
    "    print(f\"Prompt: {custom_prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        prediction = test_model_inference(\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            image_path=image_path,\n",
    "            prompt=custom_prompt\n",
    "        )\n",
    "        return prediction\n",
    "    else:\n",
    "        print(f\"❌ Image not found: {image_path}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment and modify the path to test your own images):\n",
    "# prediction = test_custom_image(\"/kaggle/input/small-isic/ISIC_0024312.jpg\")\n",
    "# prediction = test_custom_image(\"/path/to/your/skin/image.jpg\", \"What type of skin lesion is this?\")\n",
    "\n",
    "print(\"Custom test function ready!\")\n",
    "print(\"📝 To test your own image, use:\")\n",
    "print('   test_custom_image(\"/path/to/image.jpg\", \"Your custom prompt\")')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8074728,
     "sourceId": 12772711,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
